{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Databases Databases gives you simple asyncio support for a range of databases. It allows you to make queries using the powerful SQLAlchemy Core expression language, and provides support for PostgreSQL, MySQL, and SQLite. Databases is suitable for integrating against any async Web framework, such as Starlette , Sanic , Responder , Quart , aiohttp , Tornado , or FastAPI . Requirements : Python 3.8+ Installation $ pip install databases Database drivers supported are: asyncpg aiopg aiomysql asyncmy aiosqlite You can install the required database drivers with: $ pip install databases [ asyncpg ] $ pip install databases [ aiopg ] $ pip install databases [ aiomysql ] $ pip install databases [ asyncmy ] $ pip install databases [ aiosqlite ] Note that if you are using any synchronous SQLAlchemy functions such as engine.create_all() or alembic migrations then you still have to install a synchronous DB driver: psycopg2 for PostgreSQL and pymysql for MySQL. Quickstart For this example we'll create a very simple SQLite database to run some queries against. $ pip install databases [ aiosqlite ] $ pip install ipython We can now run a simple example from the console. Note that we want to use ipython here, because it supports using await expressions directly from the console. # Create a database instance, and connect to it. from databases import Database database = Database ( 'sqlite+aiosqlite:///example.db' ) await database . connect () # Create a table. query = \"\"\"CREATE TABLE HighScores (id INTEGER PRIMARY KEY, name VARCHAR(100), score INTEGER)\"\"\" await database . execute ( query = query ) # Insert some data. query = \"INSERT INTO HighScores(name, score) VALUES (:name, :score)\" values = [ { \"name\" : \"Daisy\" , \"score\" : 92 }, { \"name\" : \"Neil\" , \"score\" : 87 }, { \"name\" : \"Carol\" , \"score\" : 43 }, ] await database . execute_many ( query = query , values = values ) # Run a database query. query = \"SELECT * FROM HighScores\" rows = await database . fetch_all ( query = query ) print ( 'High Scores:' , rows ) Check out the documentation on making database queries for examples of how to start using databases together with SQLAlchemy core expressions. \u2014 \u2b50\ufe0f \u2014 Databases is BSD licensed code. Designed & built in Brighton, England.","title":"Introduction"},{"location":"#databases","text":"Databases gives you simple asyncio support for a range of databases. It allows you to make queries using the powerful SQLAlchemy Core expression language, and provides support for PostgreSQL, MySQL, and SQLite. Databases is suitable for integrating against any async Web framework, such as Starlette , Sanic , Responder , Quart , aiohttp , Tornado , or FastAPI . Requirements : Python 3.8+","title":"Databases"},{"location":"#installation","text":"$ pip install databases Database drivers supported are: asyncpg aiopg aiomysql asyncmy aiosqlite You can install the required database drivers with: $ pip install databases [ asyncpg ] $ pip install databases [ aiopg ] $ pip install databases [ aiomysql ] $ pip install databases [ asyncmy ] $ pip install databases [ aiosqlite ] Note that if you are using any synchronous SQLAlchemy functions such as engine.create_all() or alembic migrations then you still have to install a synchronous DB driver: psycopg2 for PostgreSQL and pymysql for MySQL.","title":"Installation"},{"location":"#quickstart","text":"For this example we'll create a very simple SQLite database to run some queries against. $ pip install databases [ aiosqlite ] $ pip install ipython We can now run a simple example from the console. Note that we want to use ipython here, because it supports using await expressions directly from the console. # Create a database instance, and connect to it. from databases import Database database = Database ( 'sqlite+aiosqlite:///example.db' ) await database . connect () # Create a table. query = \"\"\"CREATE TABLE HighScores (id INTEGER PRIMARY KEY, name VARCHAR(100), score INTEGER)\"\"\" await database . execute ( query = query ) # Insert some data. query = \"INSERT INTO HighScores(name, score) VALUES (:name, :score)\" values = [ { \"name\" : \"Daisy\" , \"score\" : 92 }, { \"name\" : \"Neil\" , \"score\" : 87 }, { \"name\" : \"Carol\" , \"score\" : 43 }, ] await database . execute_many ( query = query , values = values ) # Run a database query. query = \"SELECT * FROM HighScores\" rows = await database . fetch_all ( query = query ) print ( 'High Scores:' , rows ) Check out the documentation on making database queries for examples of how to start using databases together with SQLAlchemy core expressions. \u2014 \u2b50\ufe0f \u2014 Databases is BSD licensed code. Designed & built in Brighton, England.","title":"Quickstart"},{"location":"connections_and_transactions/","text":"Connections and Transactions Databases handles database connection pooling and transaction management with minimal fuss. It'll automatically deal with acquiring and releasing connections to the pool as needed, and supports a simple transaction API that transparently handles the use of either transactions or savepoints. Connecting and disconnecting You can control the database connection pool with an async context manager: async with Database ( DATABASE_URL ) as database : ... Or by using the explicit .connect() and .disconnect() methods: database = Database ( DATABASE_URL ) await database . connect () ... await database . disconnect () Connections within this connection pool are acquired for each new asyncio.Task . If you're integrating against a web framework, then you'll probably want to hook into framework startup or shutdown events. For example, with Starlette you would use the following: @app . on_event ( \"startup\" ) async def startup (): await database . connect () @app . on_event ( \"shutdown\" ) async def shutdown (): await database . disconnect () Connection options The PostgreSQL and MySQL backends provide a few connection options for SSL and for configuring the connection pool. # Use an SSL connection. database = Database ( 'postgresql+asyncpg://localhost/example?ssl=true' ) # Use a connection pool of between 5-20 connections. database = Database ( 'mysql+aiomysql://localhost/example?min_size=5&max_size=20' ) You can also use keyword arguments to pass in any connection options. Available keyword arguments may differ between database backends. database = Database ( 'postgresql+asyncpg://localhost/example' , ssl = True , min_size = 5 , max_size = 20 ) Transactions Transactions are managed by async context blocks. A transaction can be acquired from the database connection pool: async with database . transaction (): ... It can also be acquired from a specific database connection: async with database . connection () as connection : async with connection . transaction (): ... For a lower-level transaction API: transaction = await database . transaction () try : ... except : await transaction . rollback () else : await transaction . commit () You can also use .transaction() as a function decorator on any async function: @database . transaction () async def create_users ( request ): ... Transaction state is tied to the connection used in the currently executing asynchronous task. If you would like to influence an active transaction from another task, the connection must be shared. This state is inherited by tasks that share the same connection: async def add_excitement ( connnection : databases . core . Connection , id : int ): await connection . execute ( \"UPDATE notes SET text = CONCAT(text, '!!!') WHERE id = :id\" , { \"id\" : id } ) async with Database ( database_url ) as database : async with database . transaction (): # This note won't exist until the transaction closes... await database . execute ( \"INSERT INTO notes(id, text) values (1, 'databases is cool')\" ) # ...but child tasks can use this connection now! await asyncio . create_task ( add_excitement ( database . connection (), id = 1 )) await database . fetch_val ( \"SELECT text FROM notes WHERE id=1\" ) # ^ returns: \"databases is cool!!!\" Nested transactions are fully supported, and are implemented using database savepoints: async with databases . Database ( database_url ) as db : async with db . transaction () as outer : # Do something in the outer transaction ... # Suppress to prevent influence on the outer transaction with contextlib . suppress ( ValueError ): async with db . transaction (): # Do something in the inner transaction ... raise ValueError ( 'Abort the inner transaction' ) # Observe the results of the outer transaction, # without effects from the inner transaction. await db . fetch_all ( 'SELECT * FROM ...' ) Transaction isolation-level can be specified if the driver backend supports that: async with database . transaction ( isolation = \"serializable\" ): ...","title":"Connections & Transactions"},{"location":"connections_and_transactions/#connections-and-transactions","text":"Databases handles database connection pooling and transaction management with minimal fuss. It'll automatically deal with acquiring and releasing connections to the pool as needed, and supports a simple transaction API that transparently handles the use of either transactions or savepoints.","title":"Connections and Transactions"},{"location":"connections_and_transactions/#connecting-and-disconnecting","text":"You can control the database connection pool with an async context manager: async with Database ( DATABASE_URL ) as database : ... Or by using the explicit .connect() and .disconnect() methods: database = Database ( DATABASE_URL ) await database . connect () ... await database . disconnect () Connections within this connection pool are acquired for each new asyncio.Task . If you're integrating against a web framework, then you'll probably want to hook into framework startup or shutdown events. For example, with Starlette you would use the following: @app . on_event ( \"startup\" ) async def startup (): await database . connect () @app . on_event ( \"shutdown\" ) async def shutdown (): await database . disconnect ()","title":"Connecting and disconnecting"},{"location":"connections_and_transactions/#connection-options","text":"The PostgreSQL and MySQL backends provide a few connection options for SSL and for configuring the connection pool. # Use an SSL connection. database = Database ( 'postgresql+asyncpg://localhost/example?ssl=true' ) # Use a connection pool of between 5-20 connections. database = Database ( 'mysql+aiomysql://localhost/example?min_size=5&max_size=20' ) You can also use keyword arguments to pass in any connection options. Available keyword arguments may differ between database backends. database = Database ( 'postgresql+asyncpg://localhost/example' , ssl = True , min_size = 5 , max_size = 20 )","title":"Connection options"},{"location":"connections_and_transactions/#transactions","text":"Transactions are managed by async context blocks. A transaction can be acquired from the database connection pool: async with database . transaction (): ... It can also be acquired from a specific database connection: async with database . connection () as connection : async with connection . transaction (): ... For a lower-level transaction API: transaction = await database . transaction () try : ... except : await transaction . rollback () else : await transaction . commit () You can also use .transaction() as a function decorator on any async function: @database . transaction () async def create_users ( request ): ... Transaction state is tied to the connection used in the currently executing asynchronous task. If you would like to influence an active transaction from another task, the connection must be shared. This state is inherited by tasks that share the same connection: async def add_excitement ( connnection : databases . core . Connection , id : int ): await connection . execute ( \"UPDATE notes SET text = CONCAT(text, '!!!') WHERE id = :id\" , { \"id\" : id } ) async with Database ( database_url ) as database : async with database . transaction (): # This note won't exist until the transaction closes... await database . execute ( \"INSERT INTO notes(id, text) values (1, 'databases is cool')\" ) # ...but child tasks can use this connection now! await asyncio . create_task ( add_excitement ( database . connection (), id = 1 )) await database . fetch_val ( \"SELECT text FROM notes WHERE id=1\" ) # ^ returns: \"databases is cool!!!\" Nested transactions are fully supported, and are implemented using database savepoints: async with databases . Database ( database_url ) as db : async with db . transaction () as outer : # Do something in the outer transaction ... # Suppress to prevent influence on the outer transaction with contextlib . suppress ( ValueError ): async with db . transaction (): # Do something in the inner transaction ... raise ValueError ( 'Abort the inner transaction' ) # Observe the results of the outer transaction, # without effects from the inner transaction. await db . fetch_all ( 'SELECT * FROM ...' ) Transaction isolation-level can be specified if the driver backend supports that: async with database . transaction ( isolation = \"serializable\" ): ...","title":"Transactions"},{"location":"contributing/","text":"Contibuting All contributions to databases are welcome! Issues To make it as simple as possible for us to help you, please include the following: OS python version databases version database backend (mysql, sqlite or postgresql) database driver (aiopg, aiomysql etc.) Please try to always include the above unless you're unable to install databases or know it's not relevant to your question or feature request. Pull Requests It should be quite straight forward to get started and create a Pull Request. Note Unless your change is trivial (typo, docs tweak etc.), please create an issue to discuss the change before creating a pull request. To make contributing as easy and fast as possible, you'll want to run tests and linting locally. You'll need to have python >= 3.6 (recommended 3.7+) and git installed. Getting started Clone your fork and cd into the repo directory git clone git@github.com:<your username>/databases.git cd databases Create and activate virtual env virtualenv env source env/bin/activate Install databases, dependencies and test dependencies pip install -r requirements.txt Checkout a new branch and make your changes git checkout -b my-new-feature-branch Make your changes... Contribute Formatting and linting - databases uses black for formatting, autoflake for linting and mypy for type hints check run all of those with lint script ./scripts/lint Prepare tests (basic) Set-up TEST_DATABASE_URLS env variable where you can comma separate urls for several backends The simples one is for sqlite alone: sqlite:///test.db Prepare tests (all backends) In order to run all backends you need either a docker installation on your system or all supported backends servers installed on your local machine. A sample docker configuration that reflects the CI/CD workflow of databases might be: version: '2.1' services: postgres: image: postgres:10.8 environment: POSTGRES_USER: username POSTGRES_PASSWORD: password POSTGRES_DB: testsuite ports: - 5432 :5432 mysql: image: mysql:5.7 environment: MYSQL_USER: username MYSQL_PASSWORD: password MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: testsuite ports: - 3306 :3306 3. To test all backends, the test urls need to consist of all possible drivers too, so a sample might look like following: sqlite:///test.db, sqlite+aiosqlite:///test.db, mysql+aiomysql://username:password@localhost:3306/testsuite, mysql+asyncmy://username:password@localhost:3306/testsuite, postgresql+aiopg://username:password@127.0.0.1:5432/testsuite, postgresql+asyncpg://username:password@localhost:5432/testsuite Run tests ./scripts/test Build documentation If you have changed the documentation make sure it runs successfully. You can preview the live documentation by running the following command: ./scripts/docs Commit, push, and create your pull request","title":"Contributing"},{"location":"contributing/#contibuting","text":"All contributions to databases are welcome!","title":"Contibuting"},{"location":"contributing/#issues","text":"To make it as simple as possible for us to help you, please include the following: OS python version databases version database backend (mysql, sqlite or postgresql) database driver (aiopg, aiomysql etc.) Please try to always include the above unless you're unable to install databases or know it's not relevant to your question or feature request.","title":"Issues"},{"location":"contributing/#pull-requests","text":"It should be quite straight forward to get started and create a Pull Request. Note Unless your change is trivial (typo, docs tweak etc.), please create an issue to discuss the change before creating a pull request. To make contributing as easy and fast as possible, you'll want to run tests and linting locally. You'll need to have python >= 3.6 (recommended 3.7+) and git installed.","title":"Pull Requests"},{"location":"contributing/#getting-started","text":"Clone your fork and cd into the repo directory git clone git@github.com:<your username>/databases.git cd databases Create and activate virtual env virtualenv env source env/bin/activate Install databases, dependencies and test dependencies pip install -r requirements.txt Checkout a new branch and make your changes git checkout -b my-new-feature-branch","title":"Getting started"},{"location":"contributing/#make-your-changes","text":"","title":"Make your changes..."},{"location":"contributing/#contribute","text":"Formatting and linting - databases uses black for formatting, autoflake for linting and mypy for type hints check run all of those with lint script ./scripts/lint Prepare tests (basic) Set-up TEST_DATABASE_URLS env variable where you can comma separate urls for several backends The simples one is for sqlite alone: sqlite:///test.db Prepare tests (all backends) In order to run all backends you need either a docker installation on your system or all supported backends servers installed on your local machine. A sample docker configuration that reflects the CI/CD workflow of databases might be: version: '2.1' services: postgres: image: postgres:10.8 environment: POSTGRES_USER: username POSTGRES_PASSWORD: password POSTGRES_DB: testsuite ports: - 5432 :5432 mysql: image: mysql:5.7 environment: MYSQL_USER: username MYSQL_PASSWORD: password MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: testsuite ports: - 3306 :3306 3. To test all backends, the test urls need to consist of all possible drivers too, so a sample might look like following: sqlite:///test.db, sqlite+aiosqlite:///test.db, mysql+aiomysql://username:password@localhost:3306/testsuite, mysql+asyncmy://username:password@localhost:3306/testsuite, postgresql+aiopg://username:password@127.0.0.1:5432/testsuite, postgresql+asyncpg://username:password@localhost:5432/testsuite Run tests ./scripts/test Build documentation If you have changed the documentation make sure it runs successfully. You can preview the live documentation by running the following command: ./scripts/docs Commit, push, and create your pull request","title":"Contribute"},{"location":"database_queries/","text":"Database Queries Databases supports either raw SQL, or queries build using SQLAlchemy core. Table declarations If you want to make queries using SQLAlchemy core, then you'll need to declare your tables in code. This is generally good practice in any case as makes it far easier to keep your database schema in sync with the code that's accessing it. It also allows you to use database migration tools to manage schema changes. import sqlalchemy metadata = sqlalchemy . MetaData () notes = sqlalchemy . Table ( \"notes\" , metadata , sqlalchemy . Column ( \"id\" , sqlalchemy . Integer , primary_key = True ), sqlalchemy . Column ( \"text\" , sqlalchemy . String ( length = 100 )), sqlalchemy . Column ( \"completed\" , sqlalchemy . Boolean ), ) You can use any of the SQLAlchemy column types such as sqlalchemy.JSON , or custom column types. Creating tables Databases doesn't use SQLAlchemy's engine for database access internally. The usual SQLAlchemy core way to create tables with create_all is therefore not available. To work around this you can use SQLAlchemy to compile the query to SQL and then execute it with databases: from databases import Database import sqlalchemy database = Database ( \"postgresql+asyncpg://localhost/example\" ) # Establish the connection pool await database . connect () metadata = sqlalchemy . MetaData () dialect = sqlalchemy . dialects . postgresql . dialect () # Define your table(s) notes = sqlalchemy . Table ( \"notes\" , metadata , sqlalchemy . Column ( \"id\" , sqlalchemy . Integer , primary_key = True ), sqlalchemy . Column ( \"text\" , sqlalchemy . String ( length = 100 )), sqlalchemy . Column ( \"completed\" , sqlalchemy . Boolean ), ) # Create tables for table in metadata . tables . values (): # Set `if_not_exists=False` if you want the query to throw an # exception when the table already exists schema = sqlalchemy . schema . CreateTable ( table , if_not_exists = True ) query = str ( schema . compile ( dialect = dialect )) await database . execute ( query = query ) # Close all connections in the connection pool await database . disconnect () Note that this way of creating tables is only useful for local experimentation. For serious projects, we recommend using a proper database schema migrations solution like Alembic . Queries You can now use any SQLAlchemy core queries ( official tutorial ). from databases import Database database = Database ( 'postgresql+asyncpg://localhost/example' ) # Establish the connection pool await database . connect () # Execute query = notes . insert () values = { \"text\" : \"example1\" , \"completed\" : True } await database . execute ( query = query , values = values ) # Execute many query = notes . insert () values = [ { \"text\" : \"example2\" , \"completed\" : False }, { \"text\" : \"example3\" , \"completed\" : True }, ] await database . execute_many ( query = query , values = values ) # Fetch multiple rows query = notes . select () rows = await database . fetch_all ( query = query ) # Fetch single row query = notes . select () row = await database . fetch_one ( query = query ) # Fetch single value, defaults to `column=0`. query = notes . select () value = await database . fetch_val ( query = query ) # Fetch multiple rows without loading them all into memory at once query = notes . select () async for row in database . iterate ( query = query ): ... # Close all connections in the connection pool await database . disconnect () Connections are managed as a task-local state, with driver implementations transparently using connection pooling behind the scenes. Raw queries In addition to SQLAlchemy core queries, you can also perform raw SQL queries: # Execute query = \"INSERT INTO notes(text, completed) VALUES (:text, :completed)\" values = { \"text\" : \"example1\" , \"completed\" : True } await database . execute ( query = query , values = values ) # Execute many query = \"INSERT INTO notes(text, completed) VALUES (:text, :completed)\" values = [ { \"text\" : \"example2\" , \"completed\" : False }, { \"text\" : \"example3\" , \"completed\" : True }, ] await database . execute_many ( query = query , values = values ) # Fetch multiple rows query = \"SELECT * FROM notes WHERE completed = :completed\" rows = await database . fetch_all ( query = query , values = { \"completed\" : True }) # Fetch single row query = \"SELECT * FROM notes WHERE id = :id\" result = await database . fetch_one ( query = query , values = { \"id\" : 1 }) Note that query arguments should follow the :query_arg style. Query result To keep in line with SQLAlchemy 1.4 changes query result object no longer implements a mapping interface. To access query result as a mapping you should use the _mapping property. That way you can process both SQLAlchemy Rows and databases Records from raw queries with the same function without any instance checks. query = \"SELECT * FROM notes WHERE id = :id\" result = await database . fetch_one ( query = query , values = { \"id\" : 1 }) result . id # Access field via attribute result . _mapping [ 'id' ] # Access field via mapping","title":"Database Queries"},{"location":"database_queries/#database-queries","text":"Databases supports either raw SQL, or queries build using SQLAlchemy core.","title":"Database Queries"},{"location":"database_queries/#table-declarations","text":"If you want to make queries using SQLAlchemy core, then you'll need to declare your tables in code. This is generally good practice in any case as makes it far easier to keep your database schema in sync with the code that's accessing it. It also allows you to use database migration tools to manage schema changes. import sqlalchemy metadata = sqlalchemy . MetaData () notes = sqlalchemy . Table ( \"notes\" , metadata , sqlalchemy . Column ( \"id\" , sqlalchemy . Integer , primary_key = True ), sqlalchemy . Column ( \"text\" , sqlalchemy . String ( length = 100 )), sqlalchemy . Column ( \"completed\" , sqlalchemy . Boolean ), ) You can use any of the SQLAlchemy column types such as sqlalchemy.JSON , or custom column types.","title":"Table declarations"},{"location":"database_queries/#creating-tables","text":"Databases doesn't use SQLAlchemy's engine for database access internally. The usual SQLAlchemy core way to create tables with create_all is therefore not available. To work around this you can use SQLAlchemy to compile the query to SQL and then execute it with databases: from databases import Database import sqlalchemy database = Database ( \"postgresql+asyncpg://localhost/example\" ) # Establish the connection pool await database . connect () metadata = sqlalchemy . MetaData () dialect = sqlalchemy . dialects . postgresql . dialect () # Define your table(s) notes = sqlalchemy . Table ( \"notes\" , metadata , sqlalchemy . Column ( \"id\" , sqlalchemy . Integer , primary_key = True ), sqlalchemy . Column ( \"text\" , sqlalchemy . String ( length = 100 )), sqlalchemy . Column ( \"completed\" , sqlalchemy . Boolean ), ) # Create tables for table in metadata . tables . values (): # Set `if_not_exists=False` if you want the query to throw an # exception when the table already exists schema = sqlalchemy . schema . CreateTable ( table , if_not_exists = True ) query = str ( schema . compile ( dialect = dialect )) await database . execute ( query = query ) # Close all connections in the connection pool await database . disconnect () Note that this way of creating tables is only useful for local experimentation. For serious projects, we recommend using a proper database schema migrations solution like Alembic .","title":"Creating tables"},{"location":"database_queries/#queries","text":"You can now use any SQLAlchemy core queries ( official tutorial ). from databases import Database database = Database ( 'postgresql+asyncpg://localhost/example' ) # Establish the connection pool await database . connect () # Execute query = notes . insert () values = { \"text\" : \"example1\" , \"completed\" : True } await database . execute ( query = query , values = values ) # Execute many query = notes . insert () values = [ { \"text\" : \"example2\" , \"completed\" : False }, { \"text\" : \"example3\" , \"completed\" : True }, ] await database . execute_many ( query = query , values = values ) # Fetch multiple rows query = notes . select () rows = await database . fetch_all ( query = query ) # Fetch single row query = notes . select () row = await database . fetch_one ( query = query ) # Fetch single value, defaults to `column=0`. query = notes . select () value = await database . fetch_val ( query = query ) # Fetch multiple rows without loading them all into memory at once query = notes . select () async for row in database . iterate ( query = query ): ... # Close all connections in the connection pool await database . disconnect () Connections are managed as a task-local state, with driver implementations transparently using connection pooling behind the scenes.","title":"Queries"},{"location":"database_queries/#raw-queries","text":"In addition to SQLAlchemy core queries, you can also perform raw SQL queries: # Execute query = \"INSERT INTO notes(text, completed) VALUES (:text, :completed)\" values = { \"text\" : \"example1\" , \"completed\" : True } await database . execute ( query = query , values = values ) # Execute many query = \"INSERT INTO notes(text, completed) VALUES (:text, :completed)\" values = [ { \"text\" : \"example2\" , \"completed\" : False }, { \"text\" : \"example3\" , \"completed\" : True }, ] await database . execute_many ( query = query , values = values ) # Fetch multiple rows query = \"SELECT * FROM notes WHERE completed = :completed\" rows = await database . fetch_all ( query = query , values = { \"completed\" : True }) # Fetch single row query = \"SELECT * FROM notes WHERE id = :id\" result = await database . fetch_one ( query = query , values = { \"id\" : 1 }) Note that query arguments should follow the :query_arg style.","title":"Raw queries"},{"location":"database_queries/#query-result","text":"To keep in line with SQLAlchemy 1.4 changes query result object no longer implements a mapping interface. To access query result as a mapping you should use the _mapping property. That way you can process both SQLAlchemy Rows and databases Records from raw queries with the same function without any instance checks. query = \"SELECT * FROM notes WHERE id = :id\" result = await database . fetch_one ( query = query , values = { \"id\" : 1 }) result . id # Access field via attribute result . _mapping [ 'id' ] # Access field via mapping","title":"Query result"},{"location":"tests_and_migrations/","text":"Tests and Migrations Databases is designed to allow you to fully integrate with production ready services, with API support for test isolation, and integration with Alembic for database migrations. Test isolation For strict test isolation you will always want to rollback the test database to a clean state between each test case: database = Database ( DATABASE_URL , force_rollback = True ) This will ensure that all database connections are run within a transaction that rollbacks once the database is disconnected. If you're integrating against a web framework you'll typically want to use something like the following pattern: if TESTING : database = Database ( TEST_DATABASE_URL , force_rollback = True ) else : database = Database ( DATABASE_URL ) This will give you test cases that run against a different database to the development database, with strict test isolation so long as you make sure to connect and disconnect to the database between test cases. For a lower level API you can explicitly create force-rollback transactions: async with database . transaction ( force_rollback = True ): ... Migrations Because databases uses SQLAlchemy core, you can integrate with Alembic for database migration support. $ pip install alembic $ alembic init migrations You'll want to set things up so that Alembic references the configured DATABASE_URL , and uses your table metadata. In alembic.ini remove the following line: sqlalchemy.url = driver://user:pass@localhost/dbname In migrations/env.py , you need to set the 'sqlalchemy.url' configuration key, and the target_metadata variable. You'll want something like this: # The Alembic Config object. config = context . config # Configure Alembic to use our DATABASE_URL and our table definitions. # These are just examples - the exact setup will depend on whatever # framework you're integrating against. from myapp.settings import DATABASE_URL from myapp.tables import metadata config . set_main_option ( 'sqlalchemy.url' , str ( DATABASE_URL )) target_metadata = metadata ... Note that migrations will use a standard synchronous database driver, rather than using the async drivers that databases provides support for. This will also be the case if you're using SQLAlchemy's standard tooling, such as using metadata.create_all(engine) to setup the database tables. Note for MySQL : For MySQL you'll probably need to explicitly specify the pymysql dialect when using Alembic since the default MySQL dialect does not support Python 3. If you're using the databases.DatabaseURL datatype, you can obtain this using DATABASE_URL.replace(dialect=\"pymysql\")","title":"Tests & Migrations"},{"location":"tests_and_migrations/#tests-and-migrations","text":"Databases is designed to allow you to fully integrate with production ready services, with API support for test isolation, and integration with Alembic for database migrations.","title":"Tests and Migrations"},{"location":"tests_and_migrations/#test-isolation","text":"For strict test isolation you will always want to rollback the test database to a clean state between each test case: database = Database ( DATABASE_URL , force_rollback = True ) This will ensure that all database connections are run within a transaction that rollbacks once the database is disconnected. If you're integrating against a web framework you'll typically want to use something like the following pattern: if TESTING : database = Database ( TEST_DATABASE_URL , force_rollback = True ) else : database = Database ( DATABASE_URL ) This will give you test cases that run against a different database to the development database, with strict test isolation so long as you make sure to connect and disconnect to the database between test cases. For a lower level API you can explicitly create force-rollback transactions: async with database . transaction ( force_rollback = True ): ...","title":"Test isolation"},{"location":"tests_and_migrations/#migrations","text":"Because databases uses SQLAlchemy core, you can integrate with Alembic for database migration support. $ pip install alembic $ alembic init migrations You'll want to set things up so that Alembic references the configured DATABASE_URL , and uses your table metadata. In alembic.ini remove the following line: sqlalchemy.url = driver://user:pass@localhost/dbname In migrations/env.py , you need to set the 'sqlalchemy.url' configuration key, and the target_metadata variable. You'll want something like this: # The Alembic Config object. config = context . config # Configure Alembic to use our DATABASE_URL and our table definitions. # These are just examples - the exact setup will depend on whatever # framework you're integrating against. from myapp.settings import DATABASE_URL from myapp.tables import metadata config . set_main_option ( 'sqlalchemy.url' , str ( DATABASE_URL )) target_metadata = metadata ... Note that migrations will use a standard synchronous database driver, rather than using the async drivers that databases provides support for. This will also be the case if you're using SQLAlchemy's standard tooling, such as using metadata.create_all(engine) to setup the database tables. Note for MySQL : For MySQL you'll probably need to explicitly specify the pymysql dialect when using Alembic since the default MySQL dialect does not support Python 3. If you're using the databases.DatabaseURL datatype, you can obtain this using DATABASE_URL.replace(dialect=\"pymysql\")","title":"Migrations"}]}